%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage[ruled]{algorithm2e}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage[noend]{algpseudocode}
\usepackage{algorithmicx}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Final Project: Smart Gomoku Agent}

\author{Tianxiao Hu \\
  School of Computer Science\\
  Fudan University\\
  {\tt txhu14@fudan.edu.cn} \\\And
  Hui Xu \\
  School of Data Science\\
  Fudan University\\
  {\tt xuhui14@fudan.edu.cn} \\\And
  Bing Zhang \\
  School of Data Science\\
  Fudan University\\
  {\tt 14307130338@fudan.edu.cn}
}

\date{\today}

\begin{document}
\maketitle
\begin{abstract}
  This is abstract.
\end{abstract}

\section{Introduction}

This project is aimed to develop a smart agent for gomoku game.
% todo

\section{Evaluation Function}
First of all, in order to quantify the properties of the current situation, we construct an evaluation function to estimate the win-probability. And naturally, the evaluation function will incorporate a great deal of the knowledge about the Gomoku game. Therefore, the evaluation can be either naive or complicated, which depends on the your understanding towards the Gomoku game. In general, the more complex the evaluation is, the slower the program will get over time.

In this section, we mainly propose two versions of the evaluation function.

\subsection{The First Version}
The general idea of the first version is fairly simple. As is known to all, the object of the game is to be the first player to achieve five pieces in a row, horizontally, vertically or diagonally. Therefore, we can focus only on the five continuous positions on the chess-board, hereinafter called "five-tuple". Generally, the chess-board is $15\times 15$, having 572 five-tuples in all. Then we can assign a score to every five-tuple based on the number of the black and white pieces in it. Here we ignore the relative position of pieces. Then the evaluation to a given situation is the sum of the score of all 572 five-tuples. Suppose we take piece "x" and opponent takes piece "o", then the score table for me is displayed as follow,
\begin{table}[h]
\centering
\begin{tabular}{c|c}
\hline
Five-Tuple&Score \\
\hline
x&15\\
xx&400\\
xxx&1800\\
xxxx&100000\\
xxxxx&10000000\\
o&-35\\
oo&-800\\
ooo&-15000\\
oooo&-800000\\
ooooo&-10000000\\
Blank&7\\
Polluted&0\\
\hline
\end{tabular}
\end{table}

\noindent\begin{small}\emph{Note the score to blank five-tuple is 7 rather than 0. This is because there is worse condition, where the five-tuple is polluted, i.e., both black and white pieces in the tuple.}\end{small}

The first version of the evaluation function is fairly simple and works rapidly. Nevertheless, after trying dozens of man-machine games, we find the agent can make some stupid mistakes, which result from its excessively simple evaluation function. Actually, the defect can be well solved by applying minimax algorithm, but at a high price of the computing resource.
\subsection{The Second Version}
Then we intend to add more knowledge to the evaluation function to make it "smarter".

In the second version of the evaluation function, we take more account of chess-types, or in other words, the relative position of the pieces.

For every non-blank position on the chess-board, we take it as the center and extend four positions to both sides horizontally, vertically and diagonally. Then we can obtain four nine-tuples. For each nine-tuple, we check its chess-type and assign a score according to the new score table. Add up these four scores and we can get the score of this position.

Finally, our score is the sum of all positions occupied by our pieces while the opponent's score is the sum of all positions occupied by his pieces. And the evaluation to current situation is the difference of our score and opponent's score.

The new score table is stated below,
\begin{table}[h]
\centering
\begin{tabular}{c|c|c}
\hline
Chess-Type&Self-Score&Opponent-Score  \\
\hline
Five&1000000&1000000\\
Alive-Four&20000&100000\\
CoDash-Four&6100&65000\\
GapDash-Four&6000&65000\\
CoAlive-Three&1100&5500\\
GapAlive-Three&1000&5000\\
Asleep-Three&300&200\\
Alive-Two&100&90\\
Asleep-Two&10&9\\
One&3&4\\
\hline
\end{tabular}
\end{table}

\noindent\begin{small}\emph{Note the names of the chess-types are fabricated by ourselves because we can't find accurate translation to these terms. If you are interest in the exact chess mode corresponding to the chess-types, please refer to the code or contact us.}\end{small}

\section{Greedy Algorithm}
Greedy algorithm is an algorithmic paradigm that follows the problem solving heuristic of making the locally optimal choice at each stage, with the hope of finding a global optimum.

For our Gomoku agent, the evaluation function is exactly the so-called problem solving heuristic.
And the implementation of the greedy search is elaborated as follows,
\begin{algorithm}[h]
\caption{Greedy Search}
\LinesNumbered
\KwIn{chess board}
\KwOut{position of move}

\For{each position (i,j) on the board}{
　　\If{position (i,j) is blank}{
　　　　suppose place my stone on (i,j)\;
        calculate my score: myScore\;
        calculate opponent score: opScore\;
        score(i,j) = myScore - opScore;
　　}

}
Return the position with the maximum score
\end{algorithm}

However, after dozens of games, we find the greedy strategy does not in general produce an optimal solution. On the one hand, owing to the limitation of the evaluation function, the evaluated score of the current situation can't describe the win-probability precisely. On the other hand, this is also the inherent defect of the greedy algorithm. But nonetheless a greedy heuristic may yield locally optimal solutions that approximate a global optimal solution in a reasonable time, which is appealing when solving many other problems.

\section{Minimax Algorithm}

\subsection{Introduction to Minimax and $\alpha$-$\beta$ Prunning}

\subsection{Performance on Different Evaluation Functions}

\subsection{Speed Optimization}

\section{Monte Carlo Tree Search}
\subsection{Introduction to MCTS and UCT}
\par Monte Carlo Tree Search (MCTS) is a tree search technique expanding the search tree based on random sampling of the search space. The application of Monte Carlo tree search in games is based on many playouts. In each playout, the game is played out to the very end by selecting moves at random. The final game result of each playout is then used to weight the nodes in the game tree so that better nodes are more likely to be chosen in future playouts. The strategy is going to have to balance playing all of the machines to gather that information, with concentrating the plays on the observed best machine. One strategy, called UCB1, does this by constructing statistical confidence intervals for each machine
\begin{displaymath}
\bar{x}_i \pm \sqrt{\frac{C\ln n}{n_i}}
\end{displaymath}
$\bar{x}_i$: the mean playout for machine \emph{i}\\
$n_i$: the number of plays of machine \emph{i} \\
$n$: the total number of plays
Then, the strategy is to pick the machine with the highest upper bound each time. Upper Confidence bound applied to Trees (UCT)
is MCTS with UCB strategy.
\par For Gomoku game, MCTS starts with a chess board and walk chess randomly until the end. The process is repeated many times which eliminates the best move for the current chess board. Each round of Monte Carlo tree search consists of four steps:
\begin{figure}
\centering\includegraphics[width=3in]{1.png}
\caption{something}
\end{figure}
\begin{itemize}
\item[-] Selection:
Build the root node based on the current chess board and generate all of its child nodes. The move to use would be chosen by the UCB1 algorithm and applied to obtain the next position to be considered. 
\item[-] Expansion:
Selection would then proceed until reach a position where not all of the child positions have statistics recorded. 
\item[-] Simulation:
If the node hasn't been simulated, then do a typical Monte Carlo simulation for chess game. Else, generate a random child node for the leaf node and do the simulation.
\item[-] Backpropagation:
Update the reward of the simulation (generally 0 for lose and 1 for win) to the leaf node and its ancestor node. Meanwhile add the number of view for every node in the search path.
\end{itemize}
\par Repeat the playouts for many times until reach the search time or max search times and we can get the best move by selecting the maximum reward child node for the current root board. The pseudocode is showed as follow.


\section{Round Robin for Agents}

\section*{Acknowledgments}

The acknowledgments should go immediately before the references.

% include your own bib file like this:
\bibliographystyle{acl_natbib}
\bibliography{ref}

\appendix

\section{Supplemental Material}


\end{document}
